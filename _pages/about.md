---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm a Ph.D. student in [EECS at UC Berkeley](https://www.eecs.berkeley.edu), where I'm fortunate to be advised by [Prof. Yi Ma](https://people.eecs.berkeley.edu/~yima/) and [Prof. Jiantao Jiao](https://people.eecs.berkeley.edu/~jiantao/). I'm affiliated with [BAIR](https://bair.berkeley.edu/) and supported by a UC Berkeley College of Engineering fellowship. Prior to my PhD, I completed a [BA in CS](https://www.eecs.berkeley.edu) and [MS in EECS](https://eecs.berkeley.edu/academics/graduate/industry-programs/5yrms), also at UC Berkeley.

My research interests broadly lie in <b>developing principled methodology for large-scale deep learning</b>. I work to develop scientific and mathematical principles for deep learning, apply these principles to analyze, simplify, and improve existing methods, and build and scale new principled approaches. As such, my work tends to have a combination of theory, controlled experiments, and larger-scale experiments. I'm particularly interested in how the structure of high-dimensional data (including environmental feedback) interacts with deep learning methods, and how this impacts representation learning and generalization.


<details>
    <summary><u>Notes for undergraduate and masters students.</u></summary>
    <br/>

    <i>Note 1:</i> I'm happy to chat about research, graduate school, etc. Please send me an email and we can work out a time. Please include "[Advising Chat]" in your email title.
    <br/>
    <br/>

    <i>Note 2:</i> If you are interested in working with me on deep learning research, please send me an email with your background and specific interests (the more detailed, the better). Please mention what you would like to work on. Please include "[Research Collaboration Request]" in your email title. The ideal candidate is:
    <ul>
      <li>able and willing to invest at least 15 hours per week;</li>
      <li>highly self-sufficient;</li>
      <li>able to read and understand deep learning papers;</li>
      <li>comfortable with advanced linear algebra and probability;</li>
      <li>proficient with either PyTorch (preferred) or Jax.</li>
    </ul>
    Thank you for your understanding.
    <br/>
    <br/>

    <i>PS:</i> I get many serious inquiries about advising chats or potential research collaborations. I try my best to reply to every single one of them. If you don't see a reply after, say, a week, feel free to bump the email thread. In return, if you're writing to ask for a research collaboration, please seriously think about whether you are interested in the work and are willing to invest time on it.
</details>
<br/>

Selected Recent Work
===
<table style="border: none; width: 100%;">
  <tr style="border: none;">
    <td style="border: none; width: 50%; vertical-align: top;">
      <p><strong>White-Box Transformers via Sparse Rate Reduction: Compression is all There Is?</strong><br>
      JMLR 2024 (parts at NIPS 2023, ICLR 2024, CPAL 2024)<br>
      <a href="https://arxiv.org/abs/2311.13110">Paper</a> | <a href="https://github.com/Ma-Lab-Berkeley/CRATE">Code</a></p>
      <p><strong>Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</strong><br/>
      <a href="https://arxiv.org/abs/2410.13835">Paper</a> | <a href="https://github.com/GuoTianYu2000/Active-Dormant-Attention">Code</a>
      </p>
    </td>
    <td style="border: none; width: 50%; vertical-align: top;">
      <p><strong>Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction</strong><br>
      ICLR 2025 (Spotlight)<br>
      <a href="https://arxiv.org/abs/2412.17810">Paper</a> | <a href="https://github.com/ma-lab-berkeley/token-statistics-transformer">Code</a></p>
      <p><strong>Simplifying DINO via Coding Rate Regularization</strong><br/>
      <a href="https://arxiv.org/abs/2502.10385">Paper</a> | <a href="https://github.com/RobinWu218/SimDINO">Code</a>
      </p>
    </td>
  </tr>
</table>


<!-- Recent Updates
=====
- (February 2025) Our papers [Simplifying DINO by Coding Rate Regularization](https://arxiv.org/abs/2502.10385), [Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](https://arxiv.org/abs/2410.13835), and <u>Attention-Only Transformers via Unrolled Subspace Denoising</u> were accepted to CPAL 2025 (non-archival track).
- (January 2025) Our paper [Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction](https://arxiv.org/abs/2412.17810) was accepted (spotlight) to [ICLR 2025](https://iclr.cc/).
- (September 2024) Our paper [Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](https://arxiv.org/abs/2410.13835) was accepted (oral) to [NeurIPS 2024 M3L Workshop](https://sites.google.com/view/m3l-2024/).
- (September 2024) Our paper [Scaling White-Box Transformers for Vision](https://arxiv.org/abs/2405.20299) was accepted to [NeurIPS 2024](https://neurips.cc/).
- (May 2024) Started a summer research scientist internship at [NexusFlow](https://nexusflow.ai/).
- (May 2024) Our new comprehensive paper [White-Box Transformers via Sparse Rate Reduction: Compression is all There Is?](https://arxiv.org/abs/2311.13110), reviewing our "White-Box Transformers" line of work: deriving efficient, interpretable, and performant transformer-like architectures from first-principles information theory and signal processing, was accepted to [JMLR](https://jmlr.org/).
- (May 2024) Our paper [A Global Geometric Analysis of Maximal Coding Rate Reduction](https://arxiv.org/abs/2406.01909) was accepted to [ICML 2024](https://icml.cc/).
- (January 2024) Our paper [Masked Completion via Structured Diffusion with White-Box Transformers](https://arxiv.org/abs/2404.02446), which develops a connection between iterative denoising in diffusion models and representation learning in transformer-like deep networks, and uses it to construct a performant, efficient, and interpretable transformer-like autoencoder, was accepted to [ICLR 2024](https://iclr.cc/).
- (November 2023) Our papers [Emergence of Segmentation with Minimalistic White-Box Transformers](https://arxiv.org/abs/2308.16271), [Closed-Loop Transcription via Convolutional Sparse Coding](https://arxiv.org/abs/2302.09347), and [Masked Completion via Structured Diffusion with White-Box Transformers](https://arxiv.org/abs/2404.02446) were accepted to [CPAL 2024](https://cpal.cc/).
- (October 2023) Our paper [Emergence of Segmentation with Minimalistic White-Box Transformers](https://arxiv.org/abs/2308.16271) was accepted to [NeurIPS 2023 XAIA Workshop](https://neurips.cc/virtual/2023/workshop/66529).
- (September 2023) Our paper [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/abs/2306.01129), proposing an interpretable and parameter-efficient transformer-like architecture derived from first-principles, was accepted to [NeurIPS 2023](https://neurips.cc/).
- (August 2023) Started my Ph.D. program in EECS at UC Berkeley! -->
