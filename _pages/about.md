---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I'm a PhD student in [EECS at UC Berkeley](https://www.eecs.berkeley.edu), where I am fortunate to be advised by [Prof. Yi Ma](https://people.eecs.berkeley.edu/~yima/). Prior to this, I completed a [BA in CS](https://www.eecs.berkeley.edu) and [MS in EECS](https://eecs.berkeley.edu/academics/graduate/industry-programs/5yrms), also at UC Berkeley (Go Bears!).

My research interests broadly lie in <b>simplifying deep learning</b>. More specifically, I am interested in the following research topics:
- geometry and statistics of latent representations produced by modern deep learning methods.
- connecting modern deep learning practice to classical signal processing and statistics theory. 
- leveraging theoretical knowledge to design interpretable and principled learning algorithms.

I'm currently working on theories of representation learning and optimization for diffusion models and transformer networks, and applications to computer vision and language modeling systems. My recent work has been on other forms of theoretical representation learning. I have also worked in game-theoretic modeling, especially of multi-agent systems, and the intersection between learning and games. 

In my free time, I play basketball, chess, and TFT, and read sci-fi novels.

<!--<details>
    <summary>_Notes for undergraduate and masters students._</summary>

    __Note 1:__ I am happy to chat about my research or general advising. Please send me an email and we can work out a time. 

    __Note 2:__ If you are interested in research collaboration, please send me an email with your background and specific interests (the more detailed, the better). The recommended time investment is at least 15 hours per week. Unfortunately, right now my schedule is tight and generally does not permit consistent long-term mentoring of younger students, so some degree of self-sufficiency would be highly valued. To ensure a more fruitful collaboration, it would be best to have the technical knowledge to read and understand deep learning papers, especially theory-oriented work. Thank you for your understanding.
</details>-->

Recent Updates
=====
- September 2023: Our paper [White-Box Transformers via Sparse Rate Reduction](https://arxiv.org/abs/2306.01129) was accepted (poster) to [NeurIPS 2023](https://neurips.cc/).
- August 2023: Started my PhD program in EECS at UC Berkeley!

